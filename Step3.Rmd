---
title: "Steps"
author: "Annie Kellner"
date: "2023-05-10"
output: html_document
---

## GitHub Integration

This script uses git version control via Github. As of 10-17-2023, the remote repository is housed at https://github.com/anniekellner/Steps. The SSH associated with the repo is git@github.com:anniekellner/Steps.git.

Best practices for using this script require the following:

1. Integrate RStudio with git by following the protocol at https://happygitwithr.com/

2. PULL the script prior to running it so that you are sure to have the most current version

3. Make sure you are running the script on the 'Main' branch, unless you have express reason to use a different branch. You can check what branch you are using by either
  a) selecting the 'Git' tab in the top-right quadrant of RStudio (assuming RStudio's default layout) and ensuring you see the word "Main" on the right side of the menu bar OR
  b) opening a Terminal (tab in bottom left quadrant) that is set to Git Bash (this can be changed in the Tools menu). If you type `git branch` (without ``) and press enter, the *Main branch should appear highlighted with an asterisk
  

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

For this script, it is best to run one chunk (box of R code) at a time to make sure each is working properly. Also, some chunks produce output you will want to inspect. Output will appear below the chunk on this screen rather than in the bottom right window (results appear in the bottom right window for regular .R scripts)

*Throughout the script, instructions you need to follow will appear like this*. You can ignore code in the same color within code chunks, unless it is part of the code that explicitly requires user input. 

*The first chunks (before 'PART ONE') should not require any user input. You just need to run them to set up the script.*   

The first chunk ('setup') allows you to set options that apply to every chunk in the script)

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE, message = FALSE)
 # echo = FALSE means R will not print the code along with the results. 
```

The next chunk loads the libraries required for use with this script. 

```{r load-libraries}

library(terra) # for spatial data
library(sf)
library(tidyverse)
library(data.table)
library(scales)
library(viridis)
library(mapview) # for maps
library(tmap)
library(tmaptools)
library(leaflet)
library(climatol) # for Walter-Lieth Diagrams
library(extrafont) # for using Calibri font in plots
library(patchwork)
library(conflicted)

conflicts_prefer( # this ensures that the correct package is used when multiple packages contain the same function
  lubridate::month(),
  dplyr::first(),
  dplyr::last(),
  dplyr::filter(),
  dplyr::select(),
  lubridate::year(),
)
```

**NOTE: Only run the next chunk (clear-workspace) if necessary. Code is commented out to avoid accidentally clearing the workspace. If you un-comment to clear the workspace, please remember to reinsert the #**

NOTE: If you clear the workspace but save the environment for later use (i.e., an .RData file), spatial objects created by the {terra} package are not retained. You will need to re-run the code in the chunk {r spatial-check} even if you save all objects in the global environment.

```{r clear-workspace}

#rm(list = ls()) # clear workspace - this can be commented out as needed

```

These scripts contain objects and functions required to run the script.

*If you are running a new base (one that has not been run using this script previously), you will need to enter the following information into the character_vectors.R script.*:

  1) Shapefile name (this is usually an abbreviation for the base name, such as JBER for Joint Base Elmendorf Richardson)
  
  2) Official base name (per the example above, this would be "Joint Base Elmendorf Richardson")


This script can be found in './Code/Misc/character_vectors.R'. Once you open the script, it should be self-explanatory where to enter the information. If you do not have this information, contact Trevor Even at trevor.even@colostate.edu (or on Microsoft Teams). 



```{r functions}

# The 'source' function reads the following scripts and places associated functions into the global environment

source('./Code/Misc/character_vectors.R')
source('./Code/CEMML/Functions/Function_General.R')
source('./Code/Step3Functions.R')
source('./Code/Stars_HowTo/shift_longitude.R') # ability to switch longitude systems from (-180, 180 ) to (0, 360)

```

###############################################################################################
### PART ONE: User settings - choose AFB, AFB manager, model, scenarios, years of interest  ###
###############################################################################################

First, let's *select which base (AFB, etc.) and model you wish to run.*  To choose from a list, highlight the word `installation_names`, `models`, or `official_base_names_for_plots` (including the ``) and click 'Run' at the top right (or use the shortcut "Ctrl + Enter"). The available models correspond to numbers in the vector (e.g., the model HADGEM2-ES is [1]). 


In the code chunk below, *enter the number between the brackets ([]) that corresponds with the AFB and model of interest.* 

Official base names are used for plot titles. *If the base of interest is not listed, go into 'character_vectors.R' script ('./Code/Misc/character_vectors.R') and add the new base names to both the installation_names and official_base_names vectors and save. You will then have to re-run the following line above:*

source('./Code/Misc/character_vectors.R') This code will not run, so please re-run above

```{r enter-base-and-model}

installation <- installation_names[32] 

official_name <- official_base_names_for_plots[16] # These names serve as titles for the climatograph and Walter-Lieth plots

model <- models[8] 

cat(paste("The model you have selected is", model,". The abbreviated name for the installation shapefile is", installation, "and its official name is", official_name))

```

Because the installation-specific directory names for NOAA's historical data do not always match the names for the installation boundary shapefiles, we need to make sure we're pulling from the right folder.

```{r list-NOAA-folders}

NOAA_dir <- "N:/RStor/CEMML/ClimateChange/CC_Modeling/Data/Historical Climate Data/NOAA/"

list.files(path = NOAA_dir, pattern = installation)

```

*If there is more than one .csv file for the installation in question, please speak to Trevor Even (trevor.even@colostate.edu) to determine which to use*

*If only one file is listed or you know which .csv you need, please copy-paste the filename from above into the 'weather_station object (include the quotation marks)*

```{r NOAA-filePaths}

weather_station <- "JBER - Anchorage International.csv"

noaa_filepath <- paste0(NOAA_dir, weather_station)

# Create directory for results

noaa_resultsDir <- paste0("N:/RStor/CEMML/ClimateChange/CC_Modeling/Results/Results_NOAA_Hist/", weather_station)


if (!dir.exists(noaa_resultsDir)){
  dir.create(results_folder)}

```

```{r get-NOAA-data}

noaa <- read.csv(file = noaa_filepath)

```


We use the next chunk to set the path to the correct directory for the installation boundary (shapefile).

*If the AFB is managed by the Navy, set Navy = TRUE*
*If the AFB is managed by the Air Force, set Navy = FALSE*

```{r set-directory-to-access-installation-boundaries}

# Is the AFB managed by the Navy?

Navy = FALSE

dir_installation_boundaries <- ifelse(Navy == FALSE, # site boundary (shapefile used for clipping)
                                  "N:/RStor/CEMML/ClimateChange/CC_Modeling/Data/Spatial/Installation_Boundaries",
                            "N:/RStor/CEMML/ClimateChange/CC_Modeling/Data/Spatial/Installation_Boundaries/NAVY")

```

Now let's see what scenarios and variables are available for that model.

The next chunk checks what scenarios (subfolders) are contained in the 'Data' folder associated with the model of interest.

```{r scenarios, results='asis'}

# Directories

model_dir <- paste("N:/RStor/CEMML/ClimateChange/CC_Modeling/Data/Models/", model, sep = "") 

scenario_dirs <- list.dirs(path = model_dir, full.names = FALSE) 

cat("The scenarios (subfolders) associated with this model are:", sep = '/n') # cat() basically means 'print' in this context 

cat(paste("-", scenario_dirs), sep = "\n") # names into separate lines for readability ('\n' = line)

```

*Enter the scenarios of interest below*. Input values between the quotation marks ("") for 'baseline', 'scenario1', and 'scenario2'.

```{r enter-scenarios}

baseline <- "historical"
scenario1 <- "ssp245"
scenario2 <- "ssp585"

```

Next, let's see which variables are represented in this dataset. The variables will appear in a list below the code chunk.

```{r}

baseline_dir = paste(model_dir, baseline, sep = "/")
baseline_filenames <- list.files(baseline_dir, pattern = '.nc', full.names = FALSE) # this line might need to be adjusted if some files are .ncdf or other

variables <- unique(substr(baseline_filenames, 1,6)) # Numbers 1 and 6 refer to character indices within the file names (i.e., the first six letters of the file name)

cat("The variables in this dataset are:", sep = '\n')
cat(paste("-", variables), sep = "\n")

```

Of the variables listed, which do you want to include in your analysis? *Enter the variables names you want to include between the quotation marks.* 

```{r}

variables <- c("pr_day", "tasmax", "tasmin", "hurs_d", "sfcWin") # an example of how you would choose among the variables if desired

```

*Now, enter the years of interest for each time period*
1985-2014 (or 1976-2005 if it is LOCAv2 or MACA), 2021-2050, and 2051-2080

```{r enter-years-of-interest}

baseline_start_year <- 1985 # Start year of interest for the historical time period  
baseline_end_year <- 2014 # End year of interest for the historical time period

future1_start_year <- 2021 # Start year of interest for the first future time period 
future1_end_year <- 2050 # End year of interest for the first future time period 

future2_start_year <- 2051 # Start year of interest for the second future time period 
future2_end_year <- 2080 # End year of interest for the second future time period 

```

######################################################
######################################################

THIS CONCLUDES THE USER SETTINGS PORTION OF THE SCRIPT


The rest of the script should be able to be run automatically. From this point forward, all you need to do is press the green 'run' arrow for each chunk. DO NOT ALTER THE CODE BELOW. 


#######################################################             PART TWO: Data Extraction                 #######################################################


## HISTORICAL DATA (NOAA)

```{r NOAA-historical-data}


source("./Code/NOAA/NOAA_AllDays.R")



```









## CREATE VECTORS FOR YEAR-SCENARIO COMBOS

```{r character-vectors}

scenarios <- c(baseline, scenario1, scenario2)

scenario1_plotName <- ifelse(str_detect(scenario1, "ssp245"), "SSP2-4.5", "RCP 4.5")
scenario2_plotName <- ifelse(str_detect(scenario2, "ssp585"), "SSP5-8.5", "RCP 8.5")
  
scenario_plotNames <- c("Historical", scenario1_plotName, scenario2_plotName)

years <- c(baseline_start_year, # vector for easy reference
           baseline_end_year, 
           future1_start_year, 
           future1_end_year, 
           future2_start_year, 
           future2_end_year)

```


## SET DIRECTORY FOR RESULTS

*NOTE: Folder names may differ between the 'Data' and 'Results' folders within the Climate Modeling folder. This was the case for LOCA and may be the case for others.* 

```{r set-output-directory}

#results_folder <- if(model == "LOCA_CCSM4") {
  #paste0('N:/RStor/mindyc/afccm/Climate Modeling/Results_LOCA_V2',"/",installation,"/")
  #} else {
  #paste0('N:/RStor/mindyc/afccm/Climate Modeling/Results_',model,"/",installation,"/")
  #}

results_folder <- paste0("N:/RStor/CEMML/ClimateChange/CC_Modeling/Results/Results_",model,"/", installation) 

if (!dir.exists(results_folder)){
  dir.create(results_folder)}

```


## SUBSET FILES OF INTEREST

In this step, we are creating lists of filenames that will eventually combine into raster stacks. When this chunk is finished running, you can check the objects to make sure the correct number of years have been included. 

```{r list-filenames}

scenarios <- c(baseline, scenario1, scenario2)

baseline_yrs <- paste(seq(baseline_start_year, baseline_end_year, 1), collapse = '|')
future1_yrs <- paste(seq(future1_start_year, future1_end_year, 1), collapse = '|')
future2_yrs <- paste(seq(future2_start_year, future2_end_year, 1), collapse = '|')

files_baseline <- list() # historical
files_s1f1 <- list() # scenario1, future 1
files_s1f2 <- list() # scenario 1, future 2
files_s2f1 <- list() # scenario 2, future 1
files_s2f2 <- list() # scenario 2, future 2

# List files for Baseline Scenario

for(i in 1:length(variables)){ 
  fileNames = list.files(baseline_dir, pattern = variables[i], full.names = TRUE)
  fileNames = str_remove_all(fileNames, pattern = ".aux.xml")
  fileNames = fileNames[!duplicated(fileNames)]
  dt = data.table(fileNames, result = grepl(baseline_yrs, fileNames))
  dt2 = dplyr::filter(dt, result == TRUE)
  files_baseline[[i]] = dt2$fileNames
}

# List files for S1F1 

for(i in 1:length(variables)){ 
  dir = paste(model_dir, scenario1, sep = '/')
  fileNames = list.files(dir, pattern = variables[i], full.names = TRUE)
  fileNames = str_remove_all(fileNames, pattern = ".aux.xml")
  fileNames = fileNames[!duplicated(fileNames)]
  dt = data.table(fileNames, result = grepl(future1_yrs, fileNames))
  dt2 = dplyr::filter(dt, result == TRUE)
  files_s1f1[[i]] = dt2$fileNames
  }

# List files for S1F2

for(i in 1:length(variables)){ 
  dir = paste(model_dir, scenario1, sep = '/')
  fileNames = list.files(dir, pattern = variables[i], full.names = TRUE)
  fileNames = str_remove_all(fileNames, pattern = ".aux.xml")
  fileNames = fileNames[!duplicated(fileNames)]
  dt = data.table(fileNames, result = grepl(future2_yrs, fileNames))
  dt2 = dplyr::filter(dt, result == TRUE)
  files_s1f2[[i]] = dt2$fileNames
  }

# List files for S2F1 

for(i in 1:length(variables)){ 
  dir = paste(model_dir, scenario2, sep = '/')
  fileNames = list.files(dir, pattern = variables[i], full.names = TRUE)
  fileNames = str_remove_all(fileNames, pattern = ".aux.xml")
  fileNames = fileNames[!duplicated(fileNames)]
  dt = data.table(fileNames, result = grepl(future1_yrs, fileNames))
  dt2 = dplyr::filter(dt, result == TRUE)
  files_s2f1[[i]] = dt2$fileNames
}

# List files for S2F2

for(i in 1:length(variables)){ 
  dir = paste(model_dir, scenario2, sep = '/')
  fileNames = list.files(dir, pattern = variables[i], full.names = TRUE)
  fileNames = str_remove_all(fileNames, pattern = ".aux.xml")
  fileNames = fileNames[!duplicated(fileNames)]
  dt = data.table(fileNames, result = grepl(future2_yrs, fileNames))
  dt2 = dplyr::filter(dt, result == TRUE)
  files_s2f2[[i]] = dt2$fileNames
}

```

## SPATIAL CHECK

The next chunk checks that the spatial files are aligned and appear in the correct location on the globe. An interactive map will appear below the code chunk, and you can zoom in or out as needed to view the AFB's (the easiest way is to scroll with your mouse). You can press the 'show in new window' button (the white square with an arrow) to open the map in another page if you prefer a larger view. You may have to wait a few moments for the map to produce. *Make sure the installation boundary aligns with one or more raster cells*. If it doesn't, you might have a difference in spatial projections between the installation boundary and the raster/ncdf files.  


```{r spatial-check}

stackX <- rast(files_s1f1[[1]]) # creates raster stack for first variable in s1f1 list (using s1f1 because has fewer files than baseline)

random_raster <- as.numeric(sample(1:nlyr(stackX), 1)) # get a random raster layer from StackX to use as an example

rx <- stackX[[random_raster]]

# AFB 

afb_dir <- (paste(dir_installation_boundaries, installation, sep = '/'))
afb_filename <- paste(afb_dir, '.shp', sep = "")

afbSF <- st_read(afb_filename) # Because plotting package (tmap) doesn't take terra vectors
afb <- vect(afbSF)

afbSF <- if (xmax(rx) == 360 & xmax(afb) < 0) {st_shift_longitude(afbSF)} else afbSF # adjustment in case ncdf lon scale is 0-360

afb <- vect(afbSF)
afb <- terra::aggregate(afb)

# Crop single raster to AFB shape

rxCrop <- terra::crop(rx, afb, snap = "out") # snap = "out" shows all cells that overlap AFB polygon

# Quick interactive plot

#tmap_mode('view')

map <- tm_shape(as(rxCrop, "Raster")) + 
  tm_raster() + 
  tm_shape(afbSF) + 
  tm_borders()

lf <- tmap_leaflet(map)
lf

```

Next, we will check the size of the AFB to see whether we should use its centroid or spatial extent to extract the data we need. If the AFB fits within a single NetCDF cell, it will be much faster to extract data using a single point (the centroid). The script will assume that if the AFB is smaller than the NetCDF cell, you will want to use the centroid, and if it is larger, you will want to use the spatial extent. 

You can override the automatic by changing the 'extract_obj' object to 'afb' (spatial extent) or 'cell' (centroid). See the text below the code chunk for additional comments about interpreting and manually altering the output.      


```{r decision-to-use-shp-or-centroid-for-extraction, warning=FALSE, results='asis'}

# What is the cell size of the ncdf?

rxCellSize <- cellSize(rxCrop, unit = "km") # size of the cropped raster
ncdf_cellSize <- mean(values(rxCellSize)) # get mean cell size of ncdf (all cells are not necessarily equally sized)

# How big is the AFB?

afb_area <- expanse(afb, unit = "km")

centroid <- geom(centroids(afb, inside = TRUE)) # inside = TRUE guarantees that the centroid is contained within the boundaries of the AFB (in some cases, depending on the shape of the AFB, the true centroid may be outside)
centroid_matrix <- cbind(centroid[3], centroid[4]) # used to find cell index. [3] and [4] refer to the values in the 'centroid' object. If a downstream error occurs, make sure these values are actually x and y.

extract_obj <- afb
cell <- terra::cellFromXY(rx, centroid_matrix) # finds the cell (index) in which the centroid is located

# add an if_else statement to tell R whether to extract raster values from matrix or shapefile

extract_obj <- if(afb_area < ncdf_cellSize) cell else (afb)

afbExt <- ext(afb)

cat("The object used for data extraction is ", class(extract_obj)) 

```

*INTERPRETING THE OUTPUT ABOVE*: If the output of the following code chunk says 'numeric', the script will use the centroid. If it says 'spatVector', it will use the spatial object (afb). 

*If you want to override the automatic choice, manually highlight and run either of the options below:* (if you enter it into the code chunk it will remain there and you risk overriding future choices you may not want to, so just do it here in the main text.) 

`extract_obj <- cell` to use the centroid  -OR-
`extract_obj <- afb` to use the entire shapefile


## DATA EXTRACTION

The following chunk extracts raw data from the ncdf files of interest. It then saves the global environment to the Workspace Images folder ("N:/RStor/mindyc/afccm/Climate Modeling/Workspace Images/"). 

As written, this code assumes the centroid is being used for data extraction. It has not been tested for use with a spatial object. It may still work, but might be quite slow. If you are running several large bases sequentially, it will be worth exploring an alternative coding strategy.

This is the most time-consuming chunk of the entire script. To avoid repeating this step, the global environment is saved upon extraction. As of the time this script was written (9-26-2024), the {raster} package is unable to save objects into .RData images, so {raster} objects must be saved separately. Because The raster stack (object 'rx') is required to run the chunk 'add-derived-variables', the raster stack is saved alongside the .RData file.    

```{r extract-values}

# Create lists to store extracted values from ncdf data
# s1 = scenario1; s2 = scenario2
# f1 = future1; f2 = future2

results_baseline <- list()
results_s1f1 <- list()
results_s1f2 <- list()
results_s2f1 <- list()
results_s2f2 <- list()

# Extract data

for(i in 1:length(files_baseline)){
  r = rast(files_baseline[[i]])
  times = time(r)
  varName = varnames(r)[1]
  vals = values(crop(r, ext(afb)))
  vals = r[extract_obj]
  tVals = data.table::transpose(vals) # do not use purrr:transpose
  colnames(tVals) = varName
  results_baseline[[i]] = tVals
  names(results_baseline)[i] = varName
  results_baseline[[i]]$date = times
  rm(r)
}

for(i in 1:length(files_s1f1)){
  r = rast(files_s1f1[[i]])
  times = time(r)
  varName = varnames(r)[1]
  vals = r[extract_obj]
  tVals = data.table::transpose(vals) 
  colnames(tVals) = varName
  results_s1f1[[i]] = tVals
  names(results_s1f1)[i] = varName
  results_s1f1[[i]]$date = times
  rm(r)
}

for(i in 1:length(files_s1f2)){
  r = rast(files_s1f2[[i]])
  times = time(r)
  varName = varnames(r)[1]
  vals = r[extract_obj]
  tVals = data.table::transpose(vals) 
  colnames(tVals) = varName
  results_s1f2[[i]] = tVals
  names(results_s1f2)[i] <- varName
  results_s1f2[[i]]$date = times
  rm(r)
}

for(i in 1:length(files_s2f1)){
  r = rast(files_s2f1[[i]])
  times = time(r)
  varName = varnames(r)[1]
  vals = r[extract_obj]
  tVals = data.table::transpose(vals) 
  colnames(tVals) = varName
  results_s2f1[[i]] = tVals
  names(results_s2f1)[i] <- varName
  results_s2f1[[i]]$date = times
  rm(r)
}

for(i in 1:length(files_s2f2)){
  r = rast(files_s2f2[[i]])
  times = time(r)
  varName = varnames(r)[1]
  vals = r[extract_obj]
  tVals = data.table::transpose(vals) 
  colnames(tVals) = varName
  results_s2f2[[i]] = tVals
  names(results_s2f2)[i] <- varName
  results_s2f2[[i]]$date = times
  rm(r)
}

# Create directory and save raw extracted data along with associated raster stack in the event the installation needs to be re-run. 

# Filenames

timestamp <- format(today(), "%Y%m%d")

image_filename <-  paste(installation,model,timestamp, sep = "_")
rawData_filename <- paste0(image_filename,".RData")
rasterStack_filename <- paste0("rasterStack_",image_filename,".Rds")

# Directory for Workspace Images

rawDataDir <- paste0("N:/RStor/CEMML/ClimateChange/CC_Modeling/Analysis/Software/R/Workspace Images/",installation)

if (!dir.exists(rawDataDir)){
  dir.create(rawDataDir)}

save.image(file = paste0(rawDataDir,"/",rawData_filename)) # save workspace image to N: drive

saveRDS(rx, file = paste0(rawDataDir,"/",rasterStack_filename)) # save raster stack to N: drive

```


##############################################################################
###     PART THREE: Add Derived Variables    ###                        
##############################################################################

Typically, this script will be run immediately following data extraction (i.e., the global environment still contains all objects defined in Parts One and Two)  

## IF RUNNING IMMEDIATELY AFTER DATA EXTRACTION (i.e., the environment still contains all previously defined objects), SKIP TO THE CHUNK CALLED {allvaluesdf-dataframes} 

However, if you are re- specific segments of the script using previously extracted data, you will need to manually enter the required information and re-run Part One of the script. Then you will need to load the data from a workspace image and raster stack saved to the N:/ drive. Instructions to follow.  


#### INSTRUCTIONS FOR RUNNING FROM WORKSPACE IMAGE  #####


*1. Remove the #'s that precede the code for the following two chunks.*

*2. Enter the installation of interest where prompted in the first code chunk*

*3. Run the first chunk (get-saved-files)* This will give you the pathnames to enter for the second chunk, which will load the files. 

*3. In the second chunk (load-saved-files), copy-paste the full paths to the .RData and .Rds files where instructed* 

*4. Run the second chunk* You will receive an error about the rx file, but you can ignore it. The second line of code takes care of it. 

*5. Comment out (#) the code for both chunks after running*

*6. Proceed to the portion of the script that starts with "AVDF data frame"*

```{r get-saved-files}

#installation <- installation_names[36] # CHANGE THIS TO INSTALLATION OF INTEREST

#list.files(path = paste0("N:/RStor/CEMML/ClimateChange/CC_Modeling/Analysis/Software/R/Workspace Images/",installation), full.names = TRUE)

```


```{r load-saved-files}

# Load workspace image

#load("N:/RStor/CEMML/ClimateChange/CC_Modeling/Analysis/Software/R/Workspace Images/Edwards_AFB_v3_EastofBarstow/Edwards_AFB_v3_EastofBarstow_ACCESS-ESM1-5_20241120.RData") # INSERT COPY-PASTED PATH TO .RDATA FILE

# Load raster stack

#rx <- readRDS(file = "N:/RStor/CEMML/ClimateChange/CC_Modeling/Analysis/Software/R/Workspace Images/Edwards_AFB_v3_EastofBarstow/rasterStack_Edwards_AFB_v3_EastofBarstow_ACCESS-ESM1-5_20241120.Rds") #INSERT COPY-PASTED PATH TO .RDS FILE

```


FYI, the raster is loaded separately because as of the date this script was written (9-29-24), raster objects do not save correctly during the `save.image()` process. If the raster does not load, you will get an error about a missing 'pointer'. The above retrieval process should take care of this problem, but if not, you can also re-run the portion of the {r spatial-check} chunk through to the creation of the object 'rx' (this is a raster stack). The raster stack is required to create the avdf dataframe in the next chunk.


# AVDF data frame

This chunk reorganizes the extracted data into a more workable dataframe ("tibble", from the tidyverse). The output is equivalent to the 'AllValuesDataframe' from the original Step3 script. 

```{r allvaluesdf-dataframes, message=FALSE}

rm(list = ls(pattern = "^files")) # remove lists from the environment that aren't extracted data

lists <- Filter(function(x) is(x, "list"), mget(ls())) # reads in all remaining lists
nested_tibbles <- as_tibble(lists) # tibbles are tidy objects

avdf <- list() # results mimic allvaluesdf objects created by original script, with the exception of the 'time' column

for(i in 1:length(nested_tibbles)){
  sf_combo_nested = flatten(nested_tibbles[i]) # scenario-future combo
  sf = flatten(sf_combo_nested)
  tbl = tibble(lat = centroid_matrix[2], # will need to be changed when spatial objects are used for extraction
               lon = centroid_matrix[1],
               date = pluck(sf_combo_nested, 1, "date")) # pulls date column from first sublist
  sf_tbl = sf %>%
    as_tibble(sf, validate = NULL, .name_repair = "unique") %>% # validate argument necessary for running on cluster
    unnest(cols = where(is.numeric)) %>%
    dplyr::select(!(contains("date"))) %>%
    bind_cols(tbl) %>%
    dplyr::select(lat, 
                  lon, 
                  contains("max"), 
                  contains("min"), 
                  contains("pr"),
                  any_of("hurs"), # the 'any_of' helper function should ignore variables if they are not of interest
                  any_of("sfcWind"), # if it does not and you get an error, let me (Annie) know
                  date) %>%
    rename(c(tmax = tasmax, # these names match variable names in the functions that follow
             tmin = tasmin,
             prcp = pr))
  
  avdf[[i]] = sf_tbl
  names(avdf)[i] = names(nested_tibbles[i]) 
}    
 
```

## ADD DERIVED VARIABLES

NOTE: see comments within code. Functions assume the raw ncdf data does NOT use imperial units. 

The next chunk creates AllDays.csv files

```{r add-derived-vars, message=FALSE, warning=FALSE}

# Add derived (recalculated) variables to dataframes
# All functions are from the original LOCA_summarize.R script

AllDays <- list() # for MonthSum section

for(i in 1:length(avdf)){
  csv = avdf[[i]]
  csv = csv %>%
    mutate(PPT_mm = case_when( # if raster units are "kg-m-2 -1", convert to mm (otherwise keep as is)
      str_detect(units(rx), "kg") ~ prcp*86400,
      TRUE ~ as.numeric(prcp))) %>%
    mutate(PPT_in = RasterUnitConvert(PPT_mm, "MMtoIN")) %>%
    mutate(TMaxF = case_when(
      tmax > 200 ~ RasterUnitConvert(tmax, "KtoF"), # Because only values in Kelvin would be > 200
      TRUE ~ RasterUnitConvert(tmax, "CtoF"))) %>% # assuming if temp units are not Kelvin they are Celsius
    mutate(TMaxC = case_when(
      tmax > 200 ~ RasterUnitConvert(tmax, "KtoC"),
      TRUE ~ tmax)) %>% # assuming if temp units are not Kelvin they are Celsius
    mutate(TMinF = case_when(
      tmin > 200 ~ RasterUnitConvert(tmin, "KtoF"),
      TRUE ~ RasterUnitConvert(tmin, "CtoF"))) %>% # assuming if temp units are not Kelvin they are Celsius
    mutate(TMinC = case_when(
      tmin > 200 ~ RasterUnitConvert(tmin, "KtoC"),
      TRUE ~ tmin)) %>% # assuming if temp are not K they are C
    mutate(TMeanF = (TMaxF + TMinF)/2) %>%
    mutate(TmeanC = (TMaxC + TMinC)/2) %>%
    mutate(GDDF = RasterGDD(TMinF, TMaxF, 50, 86)) %>% # All hard-coded values are from the original CEMML script
    mutate(hotdays = Rasterhotdays(TMaxC, hottemp = 32.2)) %>% # Hard-coded values may be changed at user discretion
    mutate(colddays = Rastercolddays(TMinC, coldtemp = 0)) %>%
    mutate(wetdays = Rasterwetdays(PPT_mm, wetprecip = 50.8)) %>%
    mutate(drydays = Rasterdrydays(PPT_mm, dryprecip = 2.54)) %>%
    mutate(ftdays = RasterFTdays(TMaxC, TMinC, freezethresh = -2.2, thawthresh = 1.2)) 
  
  AllDays[[i]] = csv # new df for use with MonthSum
    names(AllDays)[i] = names(avdf[i])

    } # end creation of AllDays dataframe
  
# Write output to .csv

for(i in 1:length(AllDays)){
  csv = AllDays[[i]]
  csv_scenario = if (str_detect(names(AllDays[i]), "baseline")){
    "historical"
  } else if(str_detect(names(AllDays[i]),"s1")) {
    scenario1
  } else {
    scenario2
  }
  csv_years = if (str_detect(names(AllDays[i]), "f1")){
    paste(future1_start_year, "-",future1_end_year)
  } else if (str_detect(names(AllDays[i]), "f2")) {
    paste(future2_start_year,"-",future2_end_year)
  } else {
    paste(baseline_start_year,"-",baseline_end_year)
  }
    csv_fileName = paste(installation, csv_scenario, csv_years,"AllDays",sep = '_')
    
    # Create directory for files if it does not already exist
    
    if(!dir.exists(results_folder)){
      dir.create(results_folder)
    } else {
      dir.create(results_folder)
    }
    
    # Write csv to folder
    write_csv(csv, file = paste0(results_folder,"/",csv_fileName,".csv"))

    } # end write to csv

```

Now that the AllDays.csv files have been created, it is no longer necessary to retain space-intensive objects in the environment. We can routinely remove objects we no longer need.

```{r remove-extraneous-stuff}

rm(list=setdiff(ls(), c("installation", # These are the objects we want to keep
                        "official_name",
                        "model", 
                        "scenarios",
                        "scenario_plotNames",
                        "years",
                        "results_folder",
                        "AllDays",
                        "RasterUnitConvert")))

```

## Bioclimatic variables

```{r Biolimatic-variables, message=FALSE}

# Formerly Step 9A
# Writes Bioclimatics csv directly into appropriate results folder on N: drive

source('Code/Ecosystems_Climate_Data.R')

```

```{r remove-extraneous-stuff, warning=FALSE}

rm(list=setdiff(ls(), c("installation", # These are the objects we want to keep
                        "official_name",
                        "model", 
                        "scenarios",
                        "scenario_plotNames",
                        "years",
                        "results_folder",
                        "AllDays",
                        "RasterUnitConvert")))

```

## Climographs

This chunk creates all climograph plots and places them within the results folder under Plots/Climographs

```{r climographs, message=FALSE}

plots_dir <- paste0(results_folder,"/","Plots_and_Tables")

if (!dir.exists(plots_dir)){
  dir.create(plots_dir)}

source("./Code/Climographs.R")

```

```{r remove-extraneous-stuff}

rm(list=setdiff(ls(), c("installation", # These are the objects we want to keep
                        "official_name",
                        "model", 
                        "scenarios",
                        "scenario_plotNames",
                        "years",
                        "results_folder",
                        "AllDays",
                        "plots_dir",
                        "RasterUnitConvert")))

```

## MonthSum

The next chunk creates MonthSum.csv files

```{r MonthSum, message=FALSE}

# input: AllDays df
# output: MonthSum df and csv's

monthSum <- list()

MACA = if_else(str_detect(model, "MACA"), # this code is not yet functional, but does not disrupt the process
                     TRUE,
                    FALSE)

for(i in 1:length(AllDays)){
  df = AllDays[[i]]
  df = df %>%
    mutate(date = ymd(date)) %>%
    mutate(month = month(date)) %>%
    mutate(year = year(date)) %>%
    dplyr::select(-c(lat, lon))
  
  yearAvg = df %>%
    dplyr::select(!c('date','PPT_in', 'PPT_mm', 'GDDF')) %>% # Exclude variables for which the result is not simply a monthly average
    dplyr::select(!(contains("days"))) %>%
    group_by(year, month) %>%
    summarise(across(where(is.double), mean))
  
  Abs_TminF = df %>%
  select(date, year, month, TMinF) %>%
  group_by(year, month) %>%
  summarise(Abs_TminF = min(TMinF))
  
  sum_ppt = df %>%
  select(date, year, month, 'PPT_in', 'PPT_mm') %>%
  group_by(year, month) %>%
  summarise(across(contains('PPT'), sum))
  
  sum_days = df %>%
  select(date, year, month, contains('days')) %>%
  group_by(year, month) %>%
  summarise(across(contains('days'), sum))
  
  sum_GDDF = df %>%
    select(date, year, month, GDDF) %>%
    group_by(year, month) %>%
    summarise(GDDF = sum(GDDF))
  
  all = yearAvg %>%
  full_join(sum_days) %>%
  full_join(sum_ppt) %>%
  full_join(Abs_TminF) %>%
  full_join(sum_GDDF) %>%
  ungroup()
  
  monthAvg = all %>%
  dplyr::select(!year) %>%
  group_by(month) %>%
  summarise(across(everything(), mean)) %>%
  setNames(paste0('Avg_', names(.))) %>%
  rename(Abs_TminF = Avg_Abs_TminF) %>%
  select(Avg_month, # put in order on MonthSum csv
         Avg_PPT_in, 
         Avg_PPT_mm, 
         Avg_TMaxF, 
         Avg_TMinF, 
         Avg_TMeanF, 
         Abs_TminF, 
         any_of("Avg_hurs"),
         any_of("Avg_sfcWind"),
         Avg_GDDF,
         Avg_hotdays,
         Avg_colddays,
         Avg_wetdays,
         Avg_drydays,
         Avg_ftdays,
         )
  
  # As above, MACA-related code is a placeholder for future integration
  
  macaCols = if(MACA == "TRUE") {getMACAcols(all)} else {NULL} 
  
  monthAvg = if(MACA == "TRUE") {bind_cols(monthAvg, macaCols)} else {monthAvg} 
  
  # End of MACA code
  
  monthSum[[i]] = monthAvg # new df for export and use with DiffHist
  names(monthSum)[i] = names(AllDays[i])
}

# Add summary rows (YrAverage and YrTotals) and save for table construction

Avs_and_Totals <- list() # saving for summary table

for(i in 1:length(monthSum)){
  YrAverage = colMeans(monthSum[[i]][,2:ncol(monthSum[[i]])]) # converts columns to characters 
  YrTotals = colSums(monthSum[[i]][,2:ncol(monthSum[[i]])])
  csv = bind_rows(monthSum[[i]], YrAverage, YrTotals)
  
  # NA's 

  NAs = csv %>% # YrAverage
    filter(row_number() == 13) %>% # Because there will always be 12 months irrespective of model
    mutate(across(.cols = contains("PPT"), ~na_if(.,.))) %>%
    mutate(across(.cols = contains("days"), ~na_if(.,.))) %>%
    mutate(across(.cols = contains("Abs"), ~na_if(.,.))) %>%
    mutate(across(.cols = contains("GDDF"), ~na_if(.,.))) 
  
  NAs_Totals = csv %>% # YrTotals
    filter(row_number() == 14) %>%
    mutate(across(.cols = contains("Avg_T"), ~na_if(.,.))) %>%
    mutate(across(.cols = contains("Abs"), ~na_if(.,.))) %>%
    mutate(across(.cols = any_of("Avg_hurs"), ~na_if(.,.))) %>%
    mutate(across(.cols = any_of("Avg_sfcWind"), ~na_if(.,.))) 
    
  NAs_Totals = if(MACA == "TRUE") {mutate(across(.cols = .cols %in% colnames(macaCols), ~na_if(.,.)))
    } else {NAs_Totals}
  
  csv = csv %>%
    slice(1:(n()-2)) %>% # remove summary rows
    bind_rows(NAs, NAs_Totals) %>% # replace with NA's included
    rename(month = Avg_month) %>%
    mutate(month = as.character(month))
  
  csv[13,1] = "YrAverage"
  csv[14,1] = "YrTotals"
  
  Avs_and_Totals[[i]] = csv %>% slice_tail(n = 2) 
  names(Avs_and_Totals)[i] = names(AllDays[i])
  
  monthSum[[i]] = csv # will write over original monthSum df that did not have final two columns
  names(monthSum)[i] = names(AllDays[i])
}
  
# Write csv files 

for(i in 1:length(monthSum)){

csv_scenario = if (str_detect(names(monthSum[i]), "baseline")){
    "historical"
  } else if(str_detect(names(monthSum[i]),"s1")) {
    scenarios[2]
  } else {
    scenarios[3]
  }
  
  csv_years = if (str_detect(names(monthSum[i]), "f1")){
    paste(first(year(AllDays$results_s1f1$date)),"-",last(year(AllDays$results_s1f1$date)))
    
  } else if (str_detect(names(monthSum[i]), "f2")) {
    paste(first(year(AllDays$results_s1f2$date)),"-",last(year(AllDays$results_s1f2$date)))
  } else {
    paste(first(year(AllDays$results_baseline$date)),"-",last(year(AllDays$results_baseline$date)))
  }
    csv_fileName = paste(installation,csv_scenario,csv_years,"MonthSum",sep = '_')
    write_csv(monthSum[[i]], file = paste0(results_folder,"/",csv_fileName,".csv",sep = ""))
}

```

## Walter and Leith Diagrams

This section creates WL Diagrams for both F and C and places the results in the results_folder (under Plots/Walter-Lieth). The specs are set for a 4.5 x 6.5 in plot, so the appearance in the viewer does not appear aligned, but the output should be as desired. 

```{r walter-lieth-diagrams}

source('./Code/Functions/WL_Adapted.R') # Walter-Lieth plots. Adapted from https://github.com/rOpenSpain/climaemet/blob/2375dd51183d898444af2c5d8b0804cbedde2cb2/R/climatogram.R by Annie Kellner 10-15-23  
  
# Directory for WL plots
  
wl_dir <- paste(plots_dir,"Walter-Lieth", sep = "/")

if (!dir.exists(wl_dir)){
  dir.create(wl_dir)}

source('./Code/Tables_and_Plots/Walter-Lieth.R')
```


```{r remove-objects}

rm(list=setdiff(ls(), c("monthSum", 
                        "installation", 
                        "model", 
                        "scenarios",
                        "scenario_plotNames",
                        "results_folder",
                        "plots_dir",
                        "years",
                        "csv",
                        "RasterUnitConvert")))

```

## DiffHist

The next chunk writes DiffHist.csv files 

```{r DiffHist, warning=FALSE}

# input: monthSum dataframe 
# output: diffHist dataframe

diffHist <- list() # create diffHist dataframe for later use

# Create historical object

hist <- monthSum[[1]] # separate historical values for use in calculations

# Create dataframe

for(i in 2:length(monthSum)){ # 2 because [[1]] is historical 
  diff = hist %>%
  bind_rows(monthSum[[i]]) %>%
  group_by(month) %>%
  summarise_each(funs(diff(.))) %>% # this gives warnings because the function is deprecated, but it still works as of 5/9/23. If this breaks, look here first.
  slice(1,5:12, 2:4,13:14) # arranges rows in the desired order
  
  diffHist[[i-1]] = diff # add to DiffHist dataframe for future use
  names(diffHist)[[i-1]] = names(monthSum[i])
}
  
# Save csv files 

for(i in 1:length(diffHist)){

csv_scenario = if (str_detect(names(diffHist[i]), "s1")){
    scenarios[2]
  } else if(str_detect(names(diffHist[i]),"s2")) {
    scenarios[3]
  } 
  
csv_years = if (str_detect(names(diffHist[i]), "f1")){
    paste0(years[3],"-",years[4])
  } else if (str_detect(names(diffHist[i]), "f2")) {
    paste0(years[5],"-",years[6])
  } 
    
csv_fileName = paste(installation,csv_scenario,csv_years,"DiffHist",sep = '_')
write_csv(diffHist[[i]], file = paste0(results_folder,"/",csv_fileName,".csv",sep = ""))

}

```

The following chunk uses the monthSum and diffHist lists to create summary plots (bar charts).

```{r bar-charts}

bar_charts_dir <- paste(plots_dir,"Bar_Charts", sep = "/") 

if (!dir.exists(bar_charts_dir)){
  dir.create(bar_charts_dir)}

add_month <- function(df){
  df = df[1:12,]
  df = select(df, -month)
  df = mutate(df, Month = month.abb)
}


source('./Code/Tables_and_Plots/Bar_Charts/Hist_Bar_Charts.R')
source('./Code/Tables_and_Plots/Bar_Charts/Compare_Bar_Charts.R')
source('./Code/Tables_and_Plots/Bar_Charts/Format_Bar_Charts.R') # Formatting plots 

```








