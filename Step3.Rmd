---
title: "Steps"
author: "Annie Kellner"
date: "2023-05-10"
output: html_document
---

## GitHub Integration

This script uses git version control via Github. As of 10-17-2023, the remote repository is housed at https://github.com/anniekellner/Steps. The SSH associated with the repo is git@github.com:anniekellner/Steps.git.

Best practices for using this script require the following:

1. Integrate RStudio with git by following the protocol at https://happygitwithr.com/

2. PULL the script prior to running it so that you are sure to have the most current version

3. Make sure you are running the script on the 'Main' branch, unless you have express reason to use a different branch. You can check what branch you are using by either
  a) selecting the 'Git' tab in the top-right quadrant of RStudio (assuming RStudio's default layout) and ensuring you see the word "Main" on the right side of the menu bar OR
  b) opening a Terminal (tab in bottom left quadrant) that is set to Git Bash (this can be changed in the Tools menu). If you type `git branch` (without ``) and press enter, the *Main branch should appear highlighted with an asterisk
  

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

For this script, it is best to run one chunk (box of R code) at a time to make sure each is working properly. Also, some chunks produce output you will want to inspect. Output will appear below the chunk on this screen rather than in the bottom right window (results appear in the bottom right window for regular .R scripts)

*Throughout the script, instructions you need to follow will appear like this*. You can ignore code in the same color within code chunks, unless it is part of the code that explicitly requires user input. 

*The first chunks (before 'PART ONE') should not require any user input. You just need to run them to set up the script.*   

The first chunk ('setup') allows you to set options that apply to every chunk in the script)

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE, message = FALSE)
 # echo = FALSE means R will not print the code along with the results. 
```

The next chunk loads the libraries required for use with this script. 

```{r load-libraries}

library(terra) # for spatial data
library(sf)
library(tidyverse)
library(data.table)
library(scales)
#library(viridis)
library(mapview) # for maps
library(tmap)
library(tmaptools)
library(leaflet)
library(climatol) # for Walter-Lieth Diagrams
library(extrafont) # for using Calibri font in plots
library(patchwork)
library(grid) # for legend grobs (Climographs)
library(conflicted)

conflicts_prefer( # this ensures that the correct package is used when multiple packages contain the same function
  lubridate::month(),
  dplyr::first(),
  dplyr::last(),
  dplyr::filter(),
  dplyr::select(),
  lubridate::year(),
)
```

**NOTE: Only run the next chunk (clear-workspace) if necessary. Code is commented out to avoid accidentally clearing the workspace. If you un-comment to clear the workspace, please remember to reinsert the #**

NOTE: If you clear the workspace but save the environment for later use (i.e., an .RData file), spatial objects created by the {terra} package are not retained. You will need to re-run the code in the chunk {r spatial-check} even if you save all objects in the global environment.

```{r clear-workspace}

#rm(list = ls()) # clear workspace - this can be commented out as needed

```

These scripts contain objects and functions required to run the script.

*If you are running a new base (one that has not been run using this script previously), you will need to enter the following information into the character_vectors.R script.*:

  1) Shapefile name (this is usually an abbreviation for the base name, such as JBER for Joint Base Elmendorf Richardson)
  
  2) Official base name (per the example above, this would be "Joint Base Elmendorf Richardson")


This script can be found in './Code/Misc/character_vectors.R'. Once you open the script, it should be self-explanatory where to enter the information. If you do not have this information, contact Trevor Even at trevor.even@colostate.edu (or on Microsoft Teams). 



```{r functions}

# The 'source' function reads the following scripts and places associated functions into the global environment

source('./Code/Misc/character_vectors.R')
source('./Code/CEMML/Functions/Function_General.R')
source('./Code/Step3Functions.R')
source('./Code/Stars_HowTo/shift_longitude.R') # ability to switch longitude systems from (-180, 180 ) to (0, 360)

```


<<<<<<< HEAD
First, let's *select which base (AFB, etc.) and model you wish to run.*  To choose from a list, highlight the word `shp_names`, `models`, or `official_base_names_for_plots` (including the ``) and click 'Run' at the top right (or use the shortcut "Ctrl + Enter"). The available models correspond to numbers in the vector (e.g., the model HADGEM2-ES is [1]). 
=======
################################################################################
####        RUNNING FROM A WORKSPACE IMAGE                                  ####
################################################################################
>>>>>>> bb301e676f9a9ff3a77ec7470fda25f7ab8aea81


Typically, this script will be used to run new bases or new proxies for bases (i.e, running an installation or proxy location for the first time). 

<<<<<<< HEAD
Official base names are used for plot titles. *If the base of interest is not listed, go into 'character_vectors.R' script ('./Code/Misc/character_vectors.R') and add the new base names to both the shp_names and official_base_names vectors and save. You will then have to re-run the following line above:*
=======
However, sometimes you may need to re-run a base from the workspace image stored on the N:/ drive. It takes several hours to extract data for a given installation, so we save the extracted data as a workspace image (.RData file) and an associated rasterstack (.Rds file) to avoid repeating this time-consuming step. 
>>>>>>> bb301e676f9a9ff3a77ec7470fda25f7ab8aea81

## ONLY RUN THE FOLLOWING TWO CHUNKS IF YOU ARE USING STORED DATA FROM A WORKSPACE IMAGE!!! ##
## In other words, if this installation has been run previously and there are NO CHANGES to the location from which the climate data is extracted, FOLLOW DIRECTIONS FOR RUNNING THE SUBSEQUENT TWO CODE CHUNKS ##

          ###       OTHERWISE          ###

<<<<<<< HEAD
<<<<<<< HEAD
installation <- shp_names[24] 

official_name <- official_base_names_for_plots[8] # These names serve as titles for the climatograph and Walter-Lieth plots
=======
installation <- installation_names[36] 

official_name <- official_base_names_for_plots[17] # These names serve as titles for the climatograph and Walter-Lieth plots
>>>>>>> anniekellner/BarChart_Axis_Adjust
=======
## IF YOU ARE RUNNING THIS INSTALLATION (or grid cell/shapefile) FOR THE FIRST TIME, SKIP TO THE SECTION "USER SETTINGS - PART I ## 

However, if you are re- specific segments of the script using previously extracted data, you will need to manually enter the required information and re-run Part One of the script. Then you will need to load the data from a workspace image and raster stack saved to the N:/ drive. Instructions to follow.
>>>>>>> bb301e676f9a9ff3a77ec7470fda25f7ab8aea81


<<<<<<< HEAD
<<<<<<< HEAD
cat(paste("The model you have selected is", model,". The abbreviated name for the installation is", installation, "and its official name is", official_name))
=======
cat(paste("The model you have selected is", model,". The abbreviated name for the installation shapefile is", installation, "and its official name is", official_name))
>>>>>>> anniekellner/BarChart_Axis_Adjust
=======
#### INSTRUCTIONS FOR RUNNING FROM WORKSPACE IMAGE  #####


*1. Remove the #'s that precede the code for the following two chunks.*

*2. Enter the installation of interest where prompted in the first code chunk.*
    To do this, type `shp_names` into the console or highlight the inline code in blue and press ctrl+enter. 
    Then, find the number associated with the installation and enter it between the brackets where indicated.

*3. Run the first chunk ({r get-saved-files})* This will give you the pathnames to enter for the second chunk, which will load the files. 

*3. In the second chunk ({r load-saved-files}), copy-paste the full paths to the .RData and .Rds files where indicated (including parentheses)* 

*4. Run the second chunk* You will receive an error about the rx file, but you can ignore it. The second line of code takes care of it. 

*5. Comment out (#) the code for both chunks after running (as a future safeguard)*

*6. If you need to re-run the historical (NOAA) analysis, proceed to the section "OBSERVED HISTORICAL ANALYSIS"*
    *If you do NOT need to re-run the historical (NOAA) analysis, proceed to section "AUTOMATED ANALYSIS PART II -  Add Derived Variables"*
    

```{r get-saved-files}

#shp <- shp_names[28] # CHANGE THIS TO INSTALLATION OF INTEREST

#list.files(path = paste0("N:/RStor/CEMML/ClimateChange/CC_Modeling/Data/Workspace Images/",shp), full.names = TRUE)
>>>>>>> bb301e676f9a9ff3a77ec7470fda25f7ab8aea81

```


```{r load-saved-files}

# Load workspace image

#load("N:/RStor/CEMML/ClimateChange/CC_Modeling/Data/Workspace Images/Eglin_Niceville/Eglin_Niceville_EC-Earth3-Veg-LR_20250129.RData") # INSERT COPY-PASTED PATH TO .RDATA FILE

# Load raster stack

#rx <- readRDS(file = "N:/RStor/CEMML/ClimateChange/CC_Modeling/Data/Workspace Images/Eglin_Niceville/rasterStack_Eglin_Niceville_EC-Earth3-Veg-LR_20250123.Rds") #INSERT COPY-PASTED PATH TO .RDS FILE

##

```


FYI, the raster is loaded separately because as of the date this script was written (9-29-24), raster objects do not save correctly during the `save.image()` process. If the raster does not load, you will get an error about a missing 'pointer'. As stated above, the second line of code should take care of this problem. If for some reason it does not, you can also re-run the portion of the {r spatial-check} chunk through to the creation of the object 'rx' (this is a raster stack). The raster stack is required to create the avdf dataframe.


####################################################################################################
###               USER SETTINGS PART I - SELECT OR ADD INSTALLATION NAME                       #####
####################################################################################################

First, let's *select which base (AFB, etc.) you wish to run.* You will need to select or input the following:

1) *The general acronym* associated with the installation of interest (e.g., "JBLE" for "JBLE_E" or "JBLE_L"). This is required to locate the historical data associated with local weather stations, as the filenames associated with historical data are often different from those associated with the model data.  

2) *The name of the shapefile* that corresponds with the installation. For various reasons, identifying the appropriate area/grid cell for a given model can require trial-and-error, so occasionally there is more than one shapefile per base. This information will be provided by Trevor Even (trevor.even@colostate.edu)

3) *The official name of the installation*. This name is used for plot titles, and will also be provided by Trevor Even (see above) 

In some cases, you can select these names from an existing list, located in './Code/Misc/character_vectors.R'. To see the list, you can either type the following phrases into the console, or simply select the phrases below and run them (ctrl + enter). 

`base_abbreviations` (used for finding NOAA Excel files - could be an abbreviation or name [e.g., 'Eglin'])
`shp_names`           (used for locating the shapefile associated with the installation. Also identical to the name of the Results folder)
`official_base_names` (used for plot titles)

You would then identify the number associated with the installation of interest, and enter that number between the brackets in the code chunk below. (For example, Homestead_ARB would be shp_names[1], and NAVBASE San Diego would be official_base_names[2]).

The output following the code chunk will confirm your selections. You can easily revise as necessary if you entered anything incorrectly. If your installation of interest does not appear in the list, see the orange text below the code chunk.

```{r enter-base-name}

shp <- shp_names[28] 

base_abbrev <- base_abbreviations[2] 

official_name <- official_base_names[12] # These names serve as titles for the climatograph and Walter-Lieth plots


cat(paste("The official name for the installation you are running is",official_name,".","The abbreviated name for the installation shapefile is", shp,"and the acronym/shorthand name associated with the installation is", base_abbrev,"."))

```

 *If your base of interest is not listed, go into 'character_vectors.R' script ('./Code/Misc/character_vectors.R') and add the new base names to both the installation_names and official_base_names vectors. If you do not know what those entries should be, please contact Trevor Even (trevor.even@colostate.edu).*
 
*IMPORTANT: If you added new names to the vector lists, you will then need to re-run the line the imports the character vector names into the global environment. If you still do not see your new entries, return to the file and make sure to save it. Then run the line of code again. It is included below for easy reference. You can just highlight the code below and run it. You DO NOT NEED TO RE-RUN THE LINE if the appropriate names were already included in the character_vectors.R script, and you did not add new entries.*

####################################################################################################
###                USER SETTINGS PART II - SELECT OR ADD MODEL NAME                            #####
####################################################################################################

Similar to what you did above, now *enter the number between the brackets ([]) that corresponds with the model of interest.* You can type `models` into the console or highlight the blue text and press {cntrl + enter} to see the models already listed 


```{r select-model}

model <- models[7] 

cat(paste("The model you have selected is", model,"for the installation", official_name))

```


####################################################################################################
###                USER SETTINGS PART III - GET OBSERVED HISTORICAL DATA                       #####
####################################################################################################

Observed historical data comes from weather stations in close proximity to a given installation. It is frequently the case that for a single installation, data may be pulled from multiple weather stations.

The following chunk makes use of the 'shorthand' term (or acronym) for a given base (e.g., "JBLE" for JBLE-Langley or "Eglin" for "Eglin Air Force Base") to pull the filenames for all weather station data associated with a given base.

*We need to be sure we are using data from the appropriate weather station, so if more than one file shows up, please ask Trevor Even (trevor.even@colostate.edu) which to use.*


```{r list-NOAA-folders}

NOAA_dir <- "N:/RStor/CEMML/ClimateChange/CC_Modeling/Data/Historical Climate Data/NOAA/" # directory for raw data

list.files(path = NOAA_dir, pattern = base_abbrev)

```


*If only one file is listed or you know which .csv you need, please copy the filename above (including '.csv')and paste into the 'weather_station' object (include the quotation marks)*

*If more than one filename shows up, refer to Trevor as described above. Then copy-paste the appropriate filename (also described above)*


```{r pull-NOAA-data}

weather_station <-  "Eglin_NicevilleOnly.csv"

# Read in data

noaa_dataFilepath <- paste0(NOAA_dir, weather_station)

noaa <- read.csv(file = noaa_dataFilepath) # creates dataframe of observed historical data 

```

####################################################################################################
###                ANALYSES OF OBSERVED HISTORICAL DATA                                        #####
####################################################################################################

Because the historical analyses of observed data (NOAA data) use a different dataset from the rest of the script, we are running it here, prior to further user inputs. The following two chunks DO NOT require user input. Simply press the green arrow to run sections {r Output-directories} and {r Observed-historical}

## Output Directories


```{r Output-directories}

# Observed Historical - csv's

noaa_resultsDir <- paste0("N:/RStor/CEMML/ClimateChange/CC_Modeling/Results/Results_NOAA_Hist/", weather_station)

if (!dir.exists(noaa_resultsDir)){
  dir.create(noaa_resultsDir)}


# Modeled Futures

results_folder <- paste0("N:/RStor/CEMML/ClimateChange/CC_Modeling/Results/Results_",model,"/", shp) 

if (!dir.exists(results_folder)){
  dir.create(results_folder)}


# Plots and Tables - Modeled

plots_dir <- paste0(results_folder,"/","Plots_and_Tables")

if (!dir.exists(plots_dir)){
  dir.create(plots_dir)}

#  Plots and Tables - Observed Historical (NOAA climographs)

noaaClim_dir <- paste0(plots_dir,"/","NOAA_Climographs")

if (!dir.exists(noaaClim_dir)){
  dir.create(noaaClim_dir)}

```


## Analyses and plots (Climographs) for observed historical data


```{r Observed-historical}


source("./Code/NOAA/NOAA_AllDays.R")
source("./Code/NOAA/NOAA_monthSum.R")
source("./COde/NOAA/NOAA_Climographs_createDF.R")
source("./Code/NOAA/NOAA_ClimographPlots.R")


```


## Remove extraneous objects from global environment

```{r}

rm(list=setdiff(ls(), c("shp", # These are the objects we want to keep
                        "official_name",
                        "model", 
                        "plots_dir",
                        "results_folder",
                        lsf.str()))) # all functions

```



####################################################################################################
###               USER SETTINGS PART IV - ANALYSIS OF MODELED DATA                              ####
####################################################################################################


# Installation Boundaries

The following chunk enables us to access the shapefile (spatially explicit outline) of the base, or occasionally an area nearby that serves as an ecological proxy for the base. This is how we select the grid cell associated with the modeled climate data. 


The first step is to determine whether the installation is managed by the Navy (because the Navy spatial data resides in a separate directory). If you don't know the answer, ask Trevor Even (trevor.even@colostate.edu).

*If the AFB is managed by the Navy, set Navy = TRUE*
*If the AFB is NOT managed by the Navy, set Navy = FALSE* (This would include the Army, Air Force, Space Force, etc.)

```{r installation-boundaries}

# Is the AFB managed by the Navy?

Navy = FALSE

dir_installation_boundaries <- ifelse(Navy == FALSE, # site boundary (shapefile used for clipping)
<<<<<<< HEAD
                                       "/Volumes/wcnr-network-1/RStor/mindyc/afccm/AF_CIP_ENV_Data_Phase3/Installation_Boundaries/",
                                      "/Volumes/wcnr-network-1/RStor/mindyc/afccm/AF_CIP_ENV_Data_Phase3/Installation_Boundaries/NAVY/")
=======
                                  "N:/RStor/CEMML/ClimateChange/CC_Modeling/Data/Spatial/Installation_Boundaries",
                            "N:/RStor/CEMML/ClimateChange/CC_Modeling/Data/Spatial/Installation_Boundaries/NAVY")
>>>>>>> anniekellner/BarChart_Axis_Adjust

```




The next chunk checks what scenarios (subfolders) are contained in the 'Data' folder associated with the model of interest.

```{r scenarios, results='asis'}

# Directories

<<<<<<< HEAD
model_dir <- paste("/Volumes/wcnr-network-1/RStor/mindyc/afccm/Climate Modeling/Data/", model, sep = "") 
=======
model_dir <- paste("N:/RStor/CEMML/ClimateChange/CC_Modeling/Data/Models/", model, sep = "") 
>>>>>>> anniekellner/BarChart_Axis_Adjust

scenario_dirs <- list.dirs(path = model_dir, full.names = FALSE) 

cat("The scenarios (subfolders) associated with this model are:", sep = '/n') # cat() basically means 'print' in this context 

cat(paste("-", scenario_dirs), sep = "/n") # names into separate lines for readability ('/n' = line)

```

*Enter the scenarios of interest below*. Input values between the quotation marks ("") for 'baseline', 'scenario1', and 'scenario2'.

```{r enter-scenarios}

baseline <- "historical"
scenario1 <- "ssp245"
scenario2 <- "ssp585"

```

Next, let's see which variables are represented in this dataset. The variables will appear in a list below the code chunk.

```{r}

baseline_dir = paste(model_dir, baseline, sep = "/")
baseline_filenames <- list.files(baseline_dir, pattern = '.nc', full.names = FALSE) # this line might need to be adjusted if some files are .ncdf or other

variables <- unique(substr(baseline_filenames, 1,6)) # Numbers 1 and 6 refer to character indices within the file names (i.e., the first six letters of the file name)

cat("The variables in this dataset are:", sep = '\n')
cat(paste("-", variables), sep = "\n")

```

Of the variables listed, which do you want to include in your analysis? *Enter the variables names you want to include between the quotation marks.* 

```{r}

variables <- c("pr_day", "tasmax", "tasmin", "hurs_d", "sfcWin") # an example of how you would choose among the variables if desired

```

*Now, enter the years of interest for each time period*
1985-2014 (or 1976-2005 if it is LOCAv2 or MACA), 2021-2050, and 2051-2080

```{r enter-years-of-interest}

baseline_start_year <- 1985 # Start year of interest for the historical time period  
baseline_end_year <- 2014 # End year of interest for the historical time period

future1_start_year <- 2021 # Start year of interest for the first future time period 
future1_end_year <- 2050 # End year of interest for the first future time period 

future2_start_year <- 2051 # Start year of interest for the second future time period 
future2_end_year <- 2080 # End year of interest for the second future time period 

```

################################################################################################################
###               THIS CONCLUDES THE USER SETTINGS PORTION OF THE SCRIPT                                    ####
################################################################################################################


The rest of the script should be able to be run automatically. From this point forward, all you need to do is press the green 'run' arrow for each chunk. DO NOT ALTER THE CODE BELOW. 

################################################################################################################
####            AUTOMATED ANALYSIS PART I - DATA EXTRACTION                                                 ####
################################################################################################################


## CREATE VECTORS FOR YEAR-SCENARIO COMBOS

```{r character-vectors}

scenarios <- c(baseline, scenario1, scenario2)

scenario1_plotName <- ifelse(str_detect(scenario1, "ssp245"), "SSP2-4.5", "RCP 4.5")
scenario2_plotName <- ifelse(str_detect(scenario2, "ssp585"), "SSP5-8.5", "RCP 8.5")
  
scenario_plotNames <- c("Historical", scenario1_plotName, scenario2_plotName)

years <- c(baseline_start_year, # vector for easy reference
           baseline_end_year, 
           future1_start_year, 
           future1_end_year, 
           future2_start_year, 
           future2_end_year)

```


<<<<<<< HEAD
## SET DIRECTORY FOR RESULTS

*NOTE: Folder names may differ between the 'Data' and 'Results' folders within the Climate Modeling folder. This was the case for LOCA and may be the case for others.* 

```{r set-output-directory}

#results_folder <- if(model == "LOCA_CCSM4") {
<<<<<<< HEAD
  #paste0('N:\\RStor\\mindyc\\afccm\\Climate Modeling\\Results_LOCA_V2',"/",installation,"/")
  #} else {
  #paste0('N:\\RStor\\mindyc\\afccm\\Climate Modeling\\Results_',model,"/",installation,"/")
  #}

results_folder <- paste0("/Volumes/wcnr-network/RStor/mindyc/afccm/Climate Modeling/Results/", installation, "/") # MAC
=======
  #paste0('N:/RStor/mindyc/afccm/Climate Modeling/Results_LOCA_V2',"/",installation,"/")
  #} else {
  #paste0('N:/RStor/mindyc/afccm/Climate Modeling/Results_',model,"/",installation,"/")
  #}

results_folder <- paste0("N:/RStor/CEMML/ClimateChange/CC_Modeling/Results/Results_",model,"/", installation) 

if (!dir.exists(results_folder)){
  dir.create(results_folder)}

>>>>>>> anniekellner/BarChart_Axis_Adjust
```


=======
>>>>>>> bb301e676f9a9ff3a77ec7470fda25f7ab8aea81
## SUBSET FILES OF INTEREST

In this step, we are creating lists of filenames that will eventually combine into raster stacks. When this chunk is finished running, you can check the objects to make sure the correct number of years have been included. 

```{r list-filenames}

scenarios <- c(baseline, scenario1, scenario2)

baseline_yrs <- paste(seq(baseline_start_year, baseline_end_year, 1), collapse = '|')
future1_yrs <- paste(seq(future1_start_year, future1_end_year, 1), collapse = '|')
future2_yrs <- paste(seq(future2_start_year, future2_end_year, 1), collapse = '|')

files_baseline <- list() # historical
files_s1f1 <- list() # scenario1, future 1
files_s1f2 <- list() # scenario 1, future 2
files_s2f1 <- list() # scenario 2, future 1
files_s2f2 <- list() # scenario 2, future 2

# List files for Baseline Scenario

for(i in 1:length(variables)){ 
  fileNames = list.files(baseline_dir, pattern = variables[i], full.names = TRUE)
  fileNames = str_remove_all(fileNames, pattern = ".aux.xml")
  fileNames = fileNames[!duplicated(fileNames)]
  dt = data.table(fileNames, result = grepl(baseline_yrs, fileNames))
  dt2 = dplyr::filter(dt, result == TRUE)
  files_baseline[[i]] = dt2$fileNames
}

# List files for S1F1 

for(i in 1:length(variables)){ 
  dir = paste(model_dir, scenario1, sep = '/')
  fileNames = list.files(dir, pattern = variables[i], full.names = TRUE)
  fileNames = str_remove_all(fileNames, pattern = ".aux.xml")
  fileNames = fileNames[!duplicated(fileNames)]
  dt = data.table(fileNames, result = grepl(future1_yrs, fileNames))
  dt2 = dplyr::filter(dt, result == TRUE)
  files_s1f1[[i]] = dt2$fileNames
  }

# List files for S1F2

for(i in 1:length(variables)){ 
  dir = paste(model_dir, scenario1, sep = '/')
  fileNames = list.files(dir, pattern = variables[i], full.names = TRUE)
  fileNames = str_remove_all(fileNames, pattern = ".aux.xml")
  fileNames = fileNames[!duplicated(fileNames)]
  dt = data.table(fileNames, result = grepl(future2_yrs, fileNames))
  dt2 = dplyr::filter(dt, result == TRUE)
  files_s1f2[[i]] = dt2$fileNames
  }

# List files for S2F1 

for(i in 1:length(variables)){ 
  dir = paste(model_dir, scenario2, sep = '/')
  fileNames = list.files(dir, pattern = variables[i], full.names = TRUE)
  fileNames = str_remove_all(fileNames, pattern = ".aux.xml")
  fileNames = fileNames[!duplicated(fileNames)]
  dt = data.table(fileNames, result = grepl(future1_yrs, fileNames))
  dt2 = dplyr::filter(dt, result == TRUE)
  files_s2f1[[i]] = dt2$fileNames
}

# List files for S2F2

for(i in 1:length(variables)){ 
  dir = paste(model_dir, scenario2, sep = '/')
  fileNames = list.files(dir, pattern = variables[i], full.names = TRUE)
  fileNames = str_remove_all(fileNames, pattern = ".aux.xml")
  fileNames = fileNames[!duplicated(fileNames)]
  dt = data.table(fileNames, result = grepl(future2_yrs, fileNames))
  dt2 = dplyr::filter(dt, result == TRUE)
  files_s2f2[[i]] = dt2$fileNames
}

```

## SPATIAL CHECK

The next chunk checks that the spatial files are aligned and appear in the correct location on the globe. An interactive map will appear below the code chunk, and you can zoom in or out as needed to view the AFB's (the easiest way is to scroll with your mouse). You can press the 'show in new window' button (the white square with an arrow) to open the map in another page if you prefer a larger view. You may have to wait a few moments for the map to produce. *Make sure the installation boundary aligns with one or more raster cells*. If it doesn't, you might have a difference in spatial projections between the installation boundary and the raster/ncdf files.  


```{r spatial-check}

stackX <- rast(files_s1f1[[1]]) # creates raster stack for first variable in s1f1 list (using s1f1 because has fewer files than baseline)

random_raster <- as.numeric(sample(1:nlyr(stackX), 1)) # get a random raster layer from StackX to use as an example

rx <- stackX[[random_raster]]

# AFB 

afb_dir <- (paste(dir_installation_boundaries, shp, sep = '/'))
afb_filename <- paste(afb_dir, '.shp', sep = "")

afbSF <- st_read(afb_filename) # Because plotting package (tmap) doesn't take terra vectors
afb <- vect(afbSF)

afbSF <- if (xmax(rx) == 360 & xmax(afb) < 0) {st_shift_longitude(afbSF)} else afbSF # adjustment in case ncdf lon scale is 0-360

afb <- vect(afbSF)
afb <- terra::aggregate(afb)

# Crop single raster to AFB shape

rxCrop <- terra::crop(rx, afb, snap = "out") # snap = "out" shows all cells that overlap AFB polygon

# Quick interactive plot

#tmap_mode('view')

map <- tm_shape(as(rxCrop, "Raster")) + 
  tm_raster() + 
  tm_shape(afbSF) + 
  tm_borders()

lf <- tmap_leaflet(map)
lf

```

Next, we will check the size of the AFB to see whether we should use its centroid or spatial extent to extract the data we need. If the AFB fits within a single NetCDF cell, it will be much faster to extract data using a single point (the centroid). The script will assume that if the AFB is smaller than the NetCDF cell, you will want to use the centroid, and if it is larger, you will want to use the spatial extent. 

You can override the automatic by changing the 'extract_obj' object to 'afb' (spatial extent) or 'cell' (centroid). See the text below the code chunk for additional comments about interpreting and manually altering the output.      


```{r decision-to-use-shp-or-centroid-for-extraction, warning=FALSE, results='asis'}

# What is the cell size of the ncdf?

rxCellSize <- cellSize(rxCrop, unit = "km") # size of the cropped raster
ncdf_cellSize <- mean(values(rxCellSize)) # get mean cell size of ncdf (all cells are not necessarily equally sized)

# How big is the AFB?

afb_area <- expanse(afb, unit = "km")

centroid <- geom(centroids(afb, inside = TRUE)) # inside = TRUE guarantees that the centroid is contained within the boundaries of the AFB (in some cases, depending on the shape of the AFB, the true centroid may be outside)
centroid_matrix <- cbind(centroid[3], centroid[4]) # used to find cell index. [3] and [4] refer to the values in the 'centroid' object. If a downstream error occurs, make sure these values are actually x and y.

extract_obj <- afb
cell <- terra::cellFromXY(rx, centroid_matrix) # finds the cell (index) in which the centroid is located

# add an if_else statement to tell R whether to extract raster values from matrix or shapefile

extract_obj <- if(afb_area < ncdf_cellSize) cell else (afb)

afbExt <- ext(afb)

cat("The object used for data extraction is ", class(extract_obj)) 

```

*INTERPRETING THE OUTPUT ABOVE*: If the output of the following code chunk says 'numeric', the script will use the centroid. If it says 'spatVector', it will use the spatial object (afb). 

*If you want to override the automatic choice, manually highlight and run either of the options below:* (if you enter it into the code chunk it will remain there and you risk overriding future choices you may not want to, so just do it here in the main text.) 

`extract_obj <- cell` to use the centroid  -OR-
`extract_obj <- afb` to use the entire shapefile


## DATA EXTRACTION

<<<<<<< HEAD
The following chunk extracts raw data from the ncdf files of interest. It then saves the global environment to the Workspace Images folder ("N:\\RStor\\mindyc\\afccm\\Climate Modeling\\Workspace Images\\"). 
=======
The following chunk extracts raw data from the ncdf files of interest. It then saves the global environment to the Workspace Images folder ("N:/RStor/mindyc/afccm/Climate Modeling/Workspace Images/"). 
>>>>>>> anniekellner/BarChart_Axis_Adjust

As written, this code assumes the centroid is being used for data extraction. It has not been tested for use with a spatial object. It may still work, but might be quite slow. If you are running several large bases sequentially, it will be worth exploring an alternative coding strategy.

This is the most time-consuming chunk of the entire script. To avoid repeating this step, the global environment is saved upon extraction. As of the time this script was written (9-26-2024), the {raster} package is unable to save objects into .RData images, so {raster} objects must be saved separately. Because The raster stack (object 'rx') is required to run the chunk 'add-derived-variables', the raster stack is saved alongside the .RData file.    

```{r extract-values}

# Create lists to store extracted values from ncdf data
# s1 = scenario1; s2 = scenario2
# f1 = future1; f2 = future2

results_baseline <- list()
results_s1f1 <- list()
results_s1f2 <- list()
results_s2f1 <- list()
results_s2f2 <- list()

# Extract data

for(i in 1:length(files_baseline)){
  r = rast(files_baseline[[i]])
  times = time(r)
  varName = varnames(r)[1]
  vals = values(crop(r, ext(afb)))
  vals = r[extract_obj]
  tVals = data.table::transpose(vals) # do not use purrr:transpose
  colnames(tVals) = varName
  results_baseline[[i]] = tVals
  names(results_baseline)[i] = varName
  results_baseline[[i]]$date = times
  rm(r)
}

for(i in 1:length(files_s1f1)){
  r = rast(files_s1f1[[i]])
  times = time(r)
  varName = varnames(r)[1]
  vals = r[extract_obj]
  tVals = data.table::transpose(vals) 
  colnames(tVals) = varName
  results_s1f1[[i]] = tVals
  names(results_s1f1)[i] = varName
  results_s1f1[[i]]$date = times
  rm(r)
}

for(i in 1:length(files_s1f2)){
  r = rast(files_s1f2[[i]])
  times = time(r)
  varName = varnames(r)[1]
  vals = r[extract_obj]
  tVals = data.table::transpose(vals) 
  colnames(tVals) = varName
  results_s1f2[[i]] = tVals
  names(results_s1f2)[i] <- varName
  results_s1f2[[i]]$date = times
  rm(r)
}

for(i in 1:length(files_s2f1)){
  r = rast(files_s2f1[[i]])
  times = time(r)
  varName = varnames(r)[1]
  vals = r[extract_obj]
  tVals = data.table::transpose(vals) 
  colnames(tVals) = varName
  results_s2f1[[i]] = tVals
  names(results_s2f1)[i] <- varName
  results_s2f1[[i]]$date = times
  rm(r)
}

for(i in 1:length(files_s2f2)){
  r = rast(files_s2f2[[i]])
  times = time(r)
  varName = varnames(r)[1]
  vals = r[extract_obj]
  tVals = data.table::transpose(vals) 
  colnames(tVals) = varName
  results_s2f2[[i]] = tVals
  names(results_s2f2)[i] <- varName
  results_s2f2[[i]]$date = times
  rm(r)
}

# Create directory and save raw extracted data along with associated raster stack in the event the installation needs to be re-run. 

# Filenames

timestamp <- format(today(), "%Y%m%d")

image_filename <-  paste(shp,model,timestamp, sep = "_")
rawData_filename <- paste0(image_filename,".RData")
rasterStack_filename <- paste0("rasterStack_",image_filename,".Rds")

# Directory for Workspace Images

<<<<<<< HEAD
<<<<<<< HEAD
rawDataDir <- paste0("N:\\RStor\\mindyc\\afccm\\Climate Modeling\\Workspace Images\\",installation)
=======
rawDataDir <- paste0("N:/RStor/CEMML/ClimateChange/CC_Modeling/Analysis/Software/R/Workspace Images/",installation)
>>>>>>> anniekellner/BarChart_Axis_Adjust
=======
rawDataDir <- paste0("N:/RStor/CEMML/ClimateChange/CC_Modeling/Data/Workspace Images/",shp)
>>>>>>> bb301e676f9a9ff3a77ec7470fda25f7ab8aea81

if (!dir.exists(rawDataDir)){
  dir.create(rawDataDir)}

save.image(file = paste0(rawDataDir,"/",rawData_filename)) # save workspace image to N: drive

saveRDS(rx, file = paste0(rawDataDir,"/",rasterStack_filename)) # save raster stack to N: drive

```


<<<<<<< HEAD
###############################################################################################
###     PART THREE: Add Derived Variables                                 #####################  
###############################################################################################

## IF RE-RUNNING SCRIPT FROM SAVED WORKSPACE IMAGE, RUN THE NEXT TWO CHUNKS. 

## IF RUNNING IMMEDIATELY AFTER DATA EXTRACTION (i.e., the environment still contains all previously defined objects), SKIP TO THE CHUNK CALLED {allvaluesdf-dataframes} 

The two chunks that follow are commented out, assuming the chunks are being run while the global environment still contains all defined objects from the first part of the script (i.e., through data extraction)

*If the base is being re-run from the saved image (.RData file), remove the '#' and run the first chunk. This will retrieve the filenames to enter for the second chunk, which will load the files. If you are re-running the installation from an empty environment, you will need to re-enter the installation name first.*

```{r get-saved-files}

#installation <- "CapeCanaveral_45th" # CHANGE THIS TO INSTALLATION OF INTEREST

#list.files(path = paste0("N:\\RStor\\mindyc\\afccm\\Climate Modeling\\Workspace Images\\",installation), full.names = TRUE)

```

*Now, use the filenames above to load the data. You can copy-paste from the output directly into the script* I have used Cape Canaveral as an example, but remember to replace the example filenames with the output above.

```{r load-saved-files}

# Load workspace image

#load("N:\\RStor\\mindyc\\afccm\\Climate Modeling\\Workspace Images\\CapeCanaveral_45th/CapeCanaveral_45th_EC-Earth3-Veg-LR_20240927.RData")

# Load raster stack

#rx <- readRDS(file = "N:\\RStor\\mindyc\\afccm\\Climate Modeling\\Workspace Images\\CapeCanaveral_45th/rasterStack_CapeCanaveral_45th_EC-Earth3-Veg-LR_20240927.Rds")

```


*If you are re-running the installation from saved data, you will need to re-run all chunks prior to ### PART ONE ### in order to re-load the necessary libraries and functions. If not, just proceed to the {allvaluesdf-dataframes} chunk*


The next chunk reorganizes the extracted data into a more workable dataframe ("tibble", from the tidyverse). The output is equivalent to the 'AllValuesDataframe' from the original Step3 script. 

#################################################################
###       PART THREE:  ADD DERIVED VARIABLES        #############
#################################################################
=======
##############################################################################
###     AUTOMATED ANALYSIS PART II -  Add Derived Variables               ####                        
##############################################################################
>>>>>>> anniekellner/BarChart_Axis_Adjust


# AVDF data frame

This chunk reorganizes the extracted data into a more workable dataframe ("tibble", from the tidyverse). The output is equivalent to the 'AllValuesDataframe' from the original Step3 script. 

```{r allvaluesdf-dataframes, message=FALSE}

rm(list = ls(pattern = "^files")) # remove lists from the environment that aren't extracted data

lists <- Filter(function(x) is(x, "list"), mget(ls())) # reads in all remaining lists
nested_tibbles <- as_tibble(lists) # tibbles are tidy objects

avdf <- list() # results mimic allvaluesdf objects created by original script, with the exception of the 'time' column

for(i in 1:length(nested_tibbles)){
  sf_combo_nested = flatten(nested_tibbles[i]) # scenario-future combo
  sf = flatten(sf_combo_nested)
  tbl = tibble(lat = centroid_matrix[2], # will need to be changed when spatial objects are used for extraction
               lon = centroid_matrix[1],
               date = pluck(sf_combo_nested, 1, "date")) # pulls date column from first sublist
  sf_tbl = sf %>%
    as_tibble(sf, validate = NULL, .name_repair = "unique") %>% # validate argument necessary for running on cluster
    unnest(cols = where(is.numeric)) %>%
    dplyr::select(!(contains("date"))) %>%
    bind_cols(tbl) %>%
    dplyr::select(lat, 
                  lon, 
                  contains("max"), 
                  contains("min"), 
                  contains("pr"),
                  any_of("hurs"), # the 'any_of' helper function should ignore variables if they are not of interest
                  any_of("sfcWind"), # if it does not and you get an error, let me (Annie) know
                  date) %>%
    rename(c(tmax = tasmax, # these names match variable names in the functions that follow
             tmin = tasmin,
             prcp = pr))
  
  avdf[[i]] = sf_tbl
  names(avdf)[i] = names(nested_tibbles[i]) 
}    
 
```

## ADD DERIVED VARIABLES

NOTE: see comments within code. Functions assume the raw ncdf data does NOT use imperial units. 

The next chunk creates AllDays.csv files

```{r add-derived-vars, message=FALSE, warning=FALSE}

# Add derived (recalculated) variables to dataframes
# All functions are from the original LOCA_summarize.R script

AllDays <- list() # for MonthSum section

for(i in 1:length(avdf)){
  csv = avdf[[i]]
  csv = csv %>%
    mutate(PPT_mm = case_when( # if raster units are "kg-m-2 -1", convert to mm (otherwise keep as is)
      str_detect(units(rx), "kg") ~ prcp*86400,
      TRUE ~ as.numeric(prcp))) %>%
    mutate(PPT_in = RasterUnitConvert(PPT_mm, "MMtoIN")) %>%
    mutate(TMaxF = case_when(
      tmax > 200 ~ RasterUnitConvert(tmax, "KtoF"), # Because only values in Kelvin would be > 200
      TRUE ~ RasterUnitConvert(tmax, "CtoF"))) %>% # assuming if temp units are not Kelvin they are Celsius
    mutate(TMaxC = case_when(
      tmax > 200 ~ RasterUnitConvert(tmax, "KtoC"),
      TRUE ~ tmax)) %>% # assuming if temp units are not Kelvin they are Celsius
    mutate(TMinF = case_when(
      tmin > 200 ~ RasterUnitConvert(tmin, "KtoF"),
      TRUE ~ RasterUnitConvert(tmin, "CtoF"))) %>% # assuming if temp units are not Kelvin they are Celsius
    mutate(TMinC = case_when(
      tmin > 200 ~ RasterUnitConvert(tmin, "KtoC"),
      TRUE ~ tmin)) %>% # assuming if temp are not K they are C
    mutate(TMeanF = (TMaxF + TMinF)/2) %>%
    mutate(TmeanC = (TMaxC + TMinC)/2) %>%
    mutate(GDDF = RasterGDD(TMinF, TMaxF, 50, 86)) %>% # All hard-coded values are from the original CEMML script
    mutate(hotdays = Rasterhotdays(TMaxC, hottemp = 32.2)) %>% # Hard-coded values may be changed at user discretion
    mutate(colddays = Rastercolddays(TMinC, coldtemp = 0)) %>%
    mutate(wetdays = Rasterwetdays(PPT_mm, wetprecip = 50.8)) %>%
    mutate(drydays = Rasterdrydays(PPT_mm, dryprecip = 2.54)) %>%
    mutate(ftdays = RasterFTdays(TMaxC, TMinC, freezethresh = -2.2, thawthresh = 1.2)) 
  
  AllDays[[i]] = csv # new df for use with MonthSum
    names(AllDays)[i] = names(avdf[i])

    } # end creation of AllDays dataframe
  
# Write output to .csv

for(i in 1:length(AllDays)){
  csv = AllDays[[i]]
  csv_scenario = if (str_detect(names(AllDays[i]), "baseline")){
    "historical"
  } else if(str_detect(names(AllDays[i]),"s1")) {
    scenario1
  } else {
    scenario2
  }
  csv_years = if (str_detect(names(AllDays[i]), "f1")){
    paste(future1_start_year, "-",future1_end_year)
  } else if (str_detect(names(AllDays[i]), "f2")) {
    paste(future2_start_year,"-",future2_end_year)
  } else {
    paste(baseline_start_year,"-",baseline_end_year)
  }
    csv_fileName = paste(shp, csv_scenario, csv_years,"AllDays",sep = '_')
    
    # Create directory for files if it does not already exist
    
    if(!dir.exists(results_folder)){
      dir.create(results_folder)
    } else {
      dir.create(results_folder)
    }
    
    # Write csv to folder
    write_csv(csv, file = paste0(results_folder,"/",csv_fileName,".csv"))

    } # end write to csv

```

Now that the AllDays.csv files have been created, it is no longer necessary to retain space-intensive objects in the environment. We can routinely remove objects we no longer need.

```{r remove-extraneous-stuff}

rm(list=setdiff(ls(), c("shp", # These are the objects we want to keep
                        "official_name",
                        "model", 
                        "scenarios",
                        "scenario_plotNames",
                        "years",
                        "results_folder",
                        "plots_dir",
                        "AllDays",
                        "RasterUnitConvert")))

```

## Bioclimatic variables

```{r Biolimatic-variables, message=FALSE}

# Formerly Step 9A
# Writes Bioclimatics csv directly into appropriate results folder on N: drive

source('Code/Ecosystems_Climate_Data.R')

```

```{r remove-extraneous-stuff, warning=FALSE}

rm(list=setdiff(ls(), c("shp", # These are the objects we want to keep
                        "official_name",
                        "model", 
                        "scenarios",
                        "scenario_plotNames",
                        "years",
                        "results_folder",
<<<<<<< HEAD
                        "AllDays",
                        "RasterUnitConvert")))

```

## Climographs

This chunk creates all climograph plots and places them within the results folder under Plots/Climographs

```{r climographs, message=FALSE}

<<<<<<< HEAD
plots_dir <- paste0(results_folder,"Plots_and_Tables")
=======
plots_dir <- paste0(results_folder,"/","Plots_and_Tables")
>>>>>>> anniekellner/BarChart_Axis_Adjust

if (!dir.exists(plots_dir)){
  dir.create(plots_dir)}

source("./Code/Climographs.R")

```

```{r remove-extraneous-stuff}

rm(list=setdiff(ls(), c("installation", # These are the objects we want to keep
                        "official_name",
                        "model", 
                        "scenarios",
                        "scenario_plotNames",
                        "years",
                        "results_folder",
                        "AllDays",
=======
>>>>>>> bb301e676f9a9ff3a77ec7470fda25f7ab8aea81
                        "plots_dir",
                        "AllDays",
                        "RasterUnitConvert")))

```


## MonthSum

The next chunk creates MonthSum.csv files

```{r MonthSum, message=FALSE}

# input: AllDays df
# output: MonthSum df and csv's

monthSum <- list()

MACA = if_else(str_detect(model, "MACA"), # this code is not yet functional, but does not disrupt the process
                     TRUE,
                    FALSE)

for(i in 1:length(AllDays)){
  df = AllDays[[i]]
  df = df %>%
    mutate(date = ymd(date)) %>%
    mutate(month = month(date)) %>%
    mutate(year = year(date)) %>%
    dplyr::select(-c(lat, lon))
  
  yearAvg = df %>%
    dplyr::select(!c('date','PPT_in', 'PPT_mm', 'GDDF')) %>% # Exclude variables for which the result is not simply a monthly average
    dplyr::select(!(contains("days"))) %>%
    group_by(year, month) %>%
    summarise(across(where(is.numeric), mean))
  
  Abs_TminF = df %>%
  select(date, year, month, TMinF) %>%
  group_by(year, month) %>%
  summarise(Abs_TminF = min(TMinF))
  
  sum_ppt = df %>%
  select(date, year, month, 'PPT_in', 'PPT_mm') %>%
  group_by(year, month) %>%
  summarise(across(contains('PPT'), sum))
  
  sum_days = df %>%
  select(date, year, month, contains('days')) %>%
  group_by(year, month) %>%
  summarise(across(contains('days'), sum))
  
  sum_GDDF = df %>%
    select(date, year, month, GDDF) %>%
    group_by(year, month) %>%
    summarise(GDDF = sum(GDDF))
  
  all = yearAvg %>%
  full_join(sum_days) %>%
  full_join(sum_ppt) %>%
  full_join(Abs_TminF) %>%
  full_join(sum_GDDF) %>%
  ungroup()
  
  monthAvg = all %>%
  dplyr::select(!year) %>%
  group_by(month) %>%
  summarise(across(everything(), mean)) %>%
  setNames(paste0('Avg_', names(.))) %>%
  rename(Abs_TminF = Avg_Abs_TminF) %>%
  select(Avg_month, # put in order on MonthSum csv
         Avg_PPT_in, 
         Avg_PPT_mm, 
         Avg_TMaxF, 
         Avg_TMinF, 
         Avg_TMeanF, 
         Abs_TminF, 
         any_of("Avg_hurs"),
         any_of("Avg_sfcWind"),
         Avg_GDDF,
         Avg_hotdays,
         Avg_colddays,
         Avg_wetdays,
         Avg_drydays,
         Avg_ftdays
         )
  
  # As above, MACA-related code is a placeholder for future integration
  
  macaCols = if(MACA == "TRUE") {getMACAcols(all)} else {NULL} 
  
  monthAvg = if(MACA == "TRUE") {bind_cols(monthAvg, macaCols)} else {monthAvg} 
  
  # End of MACA code
  
  monthSum[[i]] = monthAvg # new df for export and use with DiffHist
  names(monthSum)[i] = names(AllDays[i])
}

# Add summary rows (YrAverage and YrTotals) and save for table construction

Avs_and_Totals <- list() # saving for summary table

for(i in 1:length(monthSum)){
  YrAverage = colMeans(monthSum[[i]][,2:ncol(monthSum[[i]])]) # converts columns to characters 
  YrTotals = colSums(monthSum[[i]][,2:ncol(monthSum[[i]])])
  csv = bind_rows(monthSum[[i]], YrAverage, YrTotals)
  
  # NA's 

  NAs = csv %>% # YrAverage
    filter(row_number() == 13) %>% # Because there will always be 12 months irrespective of model
    mutate(across(.cols = contains("PPT"), ~na_if(.,.))) %>%
    mutate(across(.cols = contains("days"), ~na_if(.,.))) %>%
    mutate(across(.cols = contains("Abs"), ~na_if(.,.))) %>%
    mutate(across(.cols = contains("GDDF"), ~na_if(.,.))) 
  
  NAs_Totals = csv %>% # YrTotals
    filter(row_number() == 14) %>%
    mutate(across(.cols = contains("Avg_T"), ~na_if(.,.))) %>%
    mutate(across(.cols = contains("Abs"), ~na_if(.,.))) %>%
    mutate(across(.cols = any_of("Avg_hurs"), ~na_if(.,.))) %>%
    mutate(across(.cols = any_of("Avg_sfcWind"), ~na_if(.,.))) 
    
  NAs_Totals = if(MACA == "TRUE") {mutate(across(.cols = .cols %in% colnames(macaCols), ~na_if(.,.)))
    } else {NAs_Totals}
  
  csv = csv %>%
    slice(1:(n()-2)) %>% # remove summary rows
    bind_rows(NAs, NAs_Totals) %>% # replace with NA's included
    rename(month = Avg_month) %>%
    mutate(month = as.character(month))
  
  csv[13,1] = "YrAverage"
  csv[14,1] = "YrTotals"
  
  Avs_and_Totals[[i]] = csv %>% slice_tail(n = 2) 
  names(Avs_and_Totals)[i] = names(AllDays[i])
  
  monthSum[[i]] = csv # will write over original monthSum df that did not have final two columns
  names(monthSum)[i] = names(AllDays[i])
}
  
# Write csv files 

for(i in 1:length(monthSum)){

csv_scenario = if (str_detect(names(monthSum[i]), "baseline")){
    "historical"
  } else if(str_detect(names(monthSum[i]),"s1")) {
    scenarios[2]
  } else {
    scenarios[3]
  }
  
  csv_years = if (str_detect(names(monthSum[i]), "f1")){
    paste(first(year(AllDays$results_s1f1$date)),"-",last(year(AllDays$results_s1f1$date)))
    
  } else if (str_detect(names(monthSum[i]), "f2")) {
    paste(first(year(AllDays$results_s1f2$date)),"-",last(year(AllDays$results_s1f2$date)))
  } else {
    paste(first(year(AllDays$results_baseline$date)),"-",last(year(AllDays$results_baseline$date)))
  }
    csv_fileName = paste(shp,csv_scenario,csv_years,"MonthSum",sep = '_')
    write_csv(monthSum[[i]], file = paste0(results_folder,"/",csv_fileName,".csv",sep = ""))
}

```


```{r remove-extraneous-objects}

rm(list=setdiff(ls(), c("shp", # These are the objects we want to keep
                        "official_name",
                        "model", 
                        "scenarios",
                        "scenario_plotNames",
                        "years",
                        "results_folder",
                        "plots_dir",
                        "AllDays",
                        "monthSum",
                        "plots_dir",
                        "RasterUnitConvert")))

```

## Climographs

This chunk creates all climograph plots and places them within the results folder under Plots/Climographs

```{r climographs, message=FALSE}


source("./Code/ClimographsDF.R")
source("./COde/Tables_and_Plots/Climograph_Plots.R")

```



## Walter and Leith Diagrams

This section creates WL Diagrams for both F and C and places the results in the results_folder (under Plots/Walter-Lieth). The specs are set for a 4.5 x 6.5 in plot, so the appearance in the viewer does not appear aligned, but the output should be as desired. 

```{r walter-lieth-diagrams}

source('./Code/Functions/WL_Adapted.R') # Walter-Lieth plots. Adapted from https://github.com/rOpenSpain/climaemet/blob/2375dd51183d898444af2c5d8b0804cbedde2cb2/R/climatogram.R by Annie Kellner 10-15-23  
  
# Directory for WL plots
  
wl_dir <- paste(plots_dir,"Walter-Lieth", sep = "/")

if (!dir.exists(wl_dir)){
  dir.create(wl_dir)}

<<<<<<< HEAD
source('./Code/Plots/Walter-Lieth.R')
=======
source('./Code/Tables_and_Plots/Walter-Lieth.R')
>>>>>>> anniekellner/BarChart_Axis_Adjust
```


```{r remove-objects}

rm(list=setdiff(ls(), c("monthSum", 
                        "shp", 
                        "model", 
                        "scenarios",
                        "scenario_plotNames",
                        "results_folder",
                        "plots_dir",
                        "years",
                        "csv",
                        "RasterUnitConvert")))

```

## DiffHist

The next chunk writes DiffHist.csv files 

```{r DiffHist, warning=FALSE}

# input: monthSum dataframe 
# output: diffHist dataframe

diffHist <- list() # create diffHist dataframe for later use

# Create historical object

hist <- monthSum[[1]] # separate historical values for use in calculations

# Create dataframe

for(i in 2:length(monthSum)){ # 2 because [[1]] is historical 
  diff = hist %>%
  bind_rows(monthSum[[i]]) %>%
  group_by(month) %>%
  summarise_each(funs(diff(.))) %>% # this gives warnings because the function is deprecated, but it still works as of 5/9/23. If this breaks, look here first.
  slice(1,5:12, 2:4,13:14) # arranges rows in the desired order
  
  diffHist[[i-1]] = diff # add to DiffHist dataframe for future use
  names(diffHist)[[i-1]] = names(monthSum[i])
}
  
# Save csv files 

for(i in 1:length(diffHist)){

csv_scenario = if (str_detect(names(diffHist[i]), "s1")){
    scenarios[2]
  } else if(str_detect(names(diffHist[i]),"s2")) {
    scenarios[3]
  } 
  
csv_years = if (str_detect(names(diffHist[i]), "f1")){
    paste0(years[3],"-",years[4])
  } else if (str_detect(names(diffHist[i]), "f2")) {
    paste0(years[5],"-",years[6])
  } 
    
csv_fileName = paste(shp,csv_scenario,csv_years,"DiffHist",sep = '_')
write_csv(diffHist[[i]], file = paste0(results_folder,"/",csv_fileName,".csv",sep = ""))

}

```

The following chunk uses the monthSum and diffHist lists to create summary plots (bar charts).

```{r bar-charts}

bar_charts_dir <- paste(plots_dir,"Bar_Charts", sep = "/") 

if (!dir.exists(bar_charts_dir)){
  dir.create(bar_charts_dir)}

add_month <- function(df){
  df = df[1:12,]
  df = select(df, -month)
  df = mutate(df, Month = month.abb)
}


source('./Code/Tables_and_Plots/Bar_Charts/Hist_Bar_Charts.R')
source('./Code/Tables_and_Plots/Bar_Charts/Compare_Bar_Charts.R')
source('./Code/Tables_and_Plots/Bar_Charts/Format_Bar_Charts.R') # Formatting plots 

```








